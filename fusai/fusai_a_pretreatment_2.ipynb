{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import csv\n",
    "import matplotlib.dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from datetime import *\n",
    "import matplotlib.pylab as pylab\n",
    "from pylab import *  \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "from sklearn.preprocessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1077175 entries, 0 to 1077174\n",
      "Data columns (total 31 columns):\n",
      "instance_id                  1077175 non-null int64\n",
      "item_id                      1077175 non-null int64\n",
      "item_category_list           1077175 non-null object\n",
      "item_property_list           1077175 non-null object\n",
      "item_brand_id                1077175 non-null int64\n",
      "item_city_id                 1077175 non-null int64\n",
      "item_price_level             1077175 non-null int64\n",
      "item_sales_level             1077175 non-null int64\n",
      "item_collected_level         1077175 non-null int64\n",
      "item_pv_level                1077175 non-null int64\n",
      "user_id                      1077175 non-null int64\n",
      "user_gender_id               1077175 non-null int64\n",
      "user_age_level               1077175 non-null int64\n",
      "user_occupation_id           1077175 non-null int64\n",
      "user_star_level              1077175 non-null int64\n",
      "context_id                   1077175 non-null int64\n",
      "context_timestamp            1077175 non-null int64\n",
      "context_page_id              1077175 non-null int64\n",
      "predict_category_property    1077175 non-null object\n",
      "shop_id                      1077175 non-null int64\n",
      "shop_review_num_level        1077175 non-null int64\n",
      "shop_review_positive_rate    1077175 non-null float64\n",
      "shop_star_level              1077175 non-null int64\n",
      "shop_score_service           1077175 non-null float64\n",
      "shop_score_delivery          1077175 non-null float64\n",
      "shop_score_description       1077175 non-null float64\n",
      "is_trade                     1077175 non-null int64\n",
      "date                         1077175 non-null object\n",
      "weekday                      1077175 non-null int64\n",
      "day                          1077175 non-null int64\n",
      "hour                         1077175 non-null int64\n",
      "dtypes: float64(4), int64(23), object(4)\n",
      "memory usage: 254.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df_3 = pd.read_csv('~/kengkeng/alimama/data/train_df_3.csv')\n",
    "print(train_df_3.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3134662 entries, 0 to 3134661\n",
      "Data columns (total 31 columns):\n",
      "instance_id                  int64\n",
      "item_id                      int64\n",
      "item_category_list           object\n",
      "item_property_list           object\n",
      "item_brand_id                int64\n",
      "item_city_id                 int64\n",
      "item_price_level             int64\n",
      "item_sales_level             int64\n",
      "item_collected_level         int64\n",
      "item_pv_level                int64\n",
      "user_id                      int64\n",
      "user_gender_id               int64\n",
      "user_age_level               int64\n",
      "user_occupation_id           int64\n",
      "user_star_level              int64\n",
      "context_id                   int64\n",
      "context_timestamp            int64\n",
      "context_page_id              int64\n",
      "predict_category_property    object\n",
      "shop_id                      int64\n",
      "shop_review_num_level        int64\n",
      "shop_review_positive_rate    float64\n",
      "shop_star_level              int64\n",
      "shop_score_service           float64\n",
      "shop_score_delivery          float64\n",
      "shop_score_description       float64\n",
      "is_trade                     int64\n",
      "date                         object\n",
      "weekday                      int64\n",
      "day                          int64\n",
      "hour                         int64\n",
      "dtypes: float64(4), int64(23), object(4)\n",
      "memory usage: 741.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df_2 = pd.read_csv('~/kengkeng/alimama/data/train_df_2.csv')\n",
    "print(train_df_2.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6220199 entries, 0 to 6220198\n",
      "Data columns (total 31 columns):\n",
      "instance_id                  int64\n",
      "item_id                      int64\n",
      "item_category_list           object\n",
      "item_property_list           object\n",
      "item_brand_id                int64\n",
      "item_city_id                 int64\n",
      "item_price_level             int64\n",
      "item_sales_level             int64\n",
      "item_collected_level         int64\n",
      "item_pv_level                int64\n",
      "user_id                      int64\n",
      "user_gender_id               int64\n",
      "user_age_level               int64\n",
      "user_occupation_id           int64\n",
      "user_star_level              int64\n",
      "context_id                   int64\n",
      "context_timestamp            int64\n",
      "context_page_id              int64\n",
      "predict_category_property    object\n",
      "shop_id                      int64\n",
      "shop_review_num_level        int64\n",
      "shop_review_positive_rate    float64\n",
      "shop_star_level              int64\n",
      "shop_score_service           float64\n",
      "shop_score_delivery          float64\n",
      "shop_score_description       float64\n",
      "is_trade                     int64\n",
      "date                         object\n",
      "weekday                      int64\n",
      "day                          int64\n",
      "hour                         int64\n",
      "dtypes: float64(4), int64(23), object(4)\n",
      "memory usage: 1.4+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df_1 = pd.read_csv('~/kengkeng/alimama/data/train_df_1.csv')\n",
    "print(train_df_1.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0db19f4bb269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtrain_df_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitMultiFea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain_df_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddContextFea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mtrain_df_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddContextFea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mtrain_df_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddContextFea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0db19f4bb269>\u001b[0m in \u001b[0;36maddContextFea\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 添加广告商品与查询词的相关性特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maddContextFea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category_property'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_cate_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_category_property\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;31m# arg is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66440)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0db19f4bb269>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 添加广告商品与查询词的相关性特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maddContextFea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category_property'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_cate_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_category_property\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 拆分多维度拼接的字段\n",
    "def splitMultiFea(df):\n",
    "    tempDf = df.drop_duplicates(subset=['item_id'])[['item_id','item_category_list','item_property_list']]\n",
    "    tempDf['item_category_list_str'] = tempDf['item_category_list'].values\n",
    "    tempDf['item_property_list_str'] = tempDf['item_property_list'].values\n",
    "    tempDf['item_category_list'] = tempDf[tempDf.item_category_list.notnull()]['item_category_list'].map(lambda x: x.split(';'))\n",
    "    tempDf['item_category0'] = tempDf[tempDf.item_category_list.notnull()]['item_category_list'].map(lambda x: x[0])\n",
    "    tempDf['item_category1'] = tempDf[tempDf.item_category_list.notnull()]['item_category_list'].map(lambda x: x[1] if len(x)>1 else np.nan)\n",
    "    tempDf['item_category2'] = tempDf[tempDf.item_category_list.notnull()]['item_category_list'].map(lambda x: x[2] if len(x)>2 else np.nan)\n",
    "    tempDf['item_property_list'] = tempDf[tempDf.item_property_list.notnull()]['item_property_list'].map(lambda x: x.split(';'))\n",
    "    df = df.drop(['item_category_list','item_property_list'], axis=1).merge(tempDf, how='left', on='item_id')\n",
    "    df['item_prop_num'] = df['item_property_list'].dropna().map(lambda x: len(x))\n",
    "    df['predict_category_property_str'] = df['predict_category_property'].values\n",
    "    df['predict_category_property'] = df[df.predict_category_property.notnull()]['predict_category_property'].map(\n",
    "        lambda x: {kv.split(':')[0]:((kv.split(':')[1].split(',') if kv.split(':')[1]!='-1' else []) if len(kv.split(':')) >= 2 else []) for kv in x.split(';')})\n",
    "    return df\n",
    "\n",
    "# 添加广告商品与查询词的相关性特征\n",
    "def addContextFea(df):\n",
    "    df['predict_category'] = df['predict_category_property'].dropna().map(lambda x: list(x.keys()))\n",
    "    df['predict_cate_num'] = df['predict_category'].dropna().map(lambda x: len(x))\n",
    "    idx = df[df.predict_category_property.notnull()].index\n",
    "    df.loc[idx,'cate_intersect_num'] = list(map(lambda x: len(np.intersect1d(x[0],x[1])), df.loc[idx, ['item_category_list','predict_category']].values))\n",
    "    df['predict_property'] = [set() for i in range(len(df))]\n",
    "    idx = df[(df.item_category2.notnull())&(df.predict_category_property.notnull())].index\n",
    "    df.loc[idx,'predict_property'] = list(map(lambda x: x[2]|set(x[1][x[0]]) if (x[0] in x[1].keys()) else x[2], df.loc[idx,['item_category2','predict_category_property','predict_property']].values))\n",
    "    idx = df[(df.item_category1.notnull())&(df.predict_category_property.notnull())].index\n",
    "    df.loc[idx,'predict_property'] = list(map(lambda x: x[2]|set(x[1][x[0]]) if (x[0] in x[1].keys()) else x[2], df.loc[idx,['item_category1','predict_category_property','predict_property']].values))\n",
    "    df['predict_property'] = df['predict_property'].map(lambda x: np.nan if len(x)==0 else list(x))\n",
    "    df['predict_prop_num'] = df[df.predict_property.notnull()]['predict_property'].map(lambda x: len(x))\n",
    "    idx = df[(df.predict_property.notnull())&(df.item_property_list.notnull())].index\n",
    "    df.loc[idx, 'prop_intersect_num'] = list(map(lambda x: len(np.intersect1d(x[0],x[1])), df.loc[idx, ['item_property_list','predict_property']].values))\n",
    "    df.loc[idx,'prop_union_num'] = list(map(lambda x: len(np.union1d(x[0],x[1])), df.loc[idx, ['item_property_list','predict_property']].values))\n",
    "    df['prop_jaccard'] = df['prop_intersect_num'] / df['prop_union_num']\n",
    "    df['prop_predict_ratio'] = df['prop_intersect_num'] / df['predict_prop_num']\n",
    "    df['prop_item_ratio'] = df['prop_intersect_num'] / df['item_prop_num']\n",
    "    df.fillna({k:-1 for k in ['predict_prop_num','prop_intersect_num','prop_union_num','prop_jaccard','prop_predict_ratio','prop_item_ratio']}, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df_3 = splitMultiFea(train_df_3)\n",
    "train_df_2 = splitMultiFea(train_df_2)\n",
    "train_df_1 = splitMultiFea(train_df_1)\n",
    "\n",
    "train_df_3 = addContextFea(train_df_3)\n",
    "train_df_2 = addContextFea(train_df_2)\n",
    "train_df_1 = addContextFea(train_df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addSet(setList):\n",
    "    allSet = []\n",
    "    for member in setList:\n",
    "        allSet = allSet + member\n",
    "    allSet = set(allSet)\n",
    "    return allSet\n",
    "\n",
    "#处理跟商品类目和属性相关的特征\n",
    "def getCategoryFuture(df):\n",
    "    df['predict_category_list'] = df['predict_category_property'].map(lambda x: [] if x == np.nan else list(kv.split(':')[0] for kv in str(x).split(';')))\n",
    "    df['predict_category_set'] = df['predict_category_list'].map(lambda x: set(x))\n",
    "    df['real_item_category_list'] = df['item_category_list'].map(lambda x: set(x))\n",
    "\n",
    "    df['predict_property_list'] = df['predict_category_property'].map(lambda x: [] if x == np.nan else ((kv.split(':')[1].split(',') if kv.split(':')[1]!='-1' else []) if len(kv.split(':')) >= 2 else [] for kv in str(x).split(';')))\n",
    "    df['predict_property_list'] = df['predict_property_list'].map(lambda x: addSet(x))\n",
    "    df['item_property_list'] = df['item_property_list'].map(lambda x: set(x))\n",
    "    return df\n",
    "\n",
    "train_df_3 = getCategoryFuture(train_df_3)\n",
    "train_df_2 = getCategoryFuture(train_df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_1 = getCategoryFuture(train_df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   match_category_proportion  match_property_proportion\n",
      "0                   0.400000                   0.333333\n",
      "1                   0.333333                   0.545455\n",
      "2                   1.000000                   0.400000\n",
      "3                   1.000000                   1.000000\n",
      "4                   0.666667                   0.200000\n",
      "5                   1.000000                   0.454545\n",
      "6                   0.250000                   1.000000\n",
      "7                   0.500000                   0.375000\n",
      "8                   0.500000                   0.000000\n",
      "9                   0.500000                   0.000000\n"
     ]
    }
   ],
   "source": [
    "def getMatchProportion(df):\n",
    "    match_category_proportion = []\n",
    "    match_property_proportion = []\n",
    "    for x,y,m,n in df[['real_item_category_list', 'predict_category_set', 'item_property_list', 'predict_property_list']].values:\n",
    "        match_category = x & y\n",
    "        match_property = m & n\n",
    "        if len(y) > 0:\n",
    "            category_proportion = len(match_category) / len(y)\n",
    "            match_category_proportion.append(category_proportion)\n",
    "        else:\n",
    "            match_category_proportion.append(0)\n",
    "        if len(n) > 0:\n",
    "            property_proportion = len(match_property) / len(n)\n",
    "            match_property_proportion.append(property_proportion)\n",
    "        else:\n",
    "            match_property_proportion.append(0)\n",
    "    df['match_category_proportion'] = match_category_proportion\n",
    "    df['match_property_proportion'] = match_property_proportion\n",
    "    return df\n",
    "\n",
    "train_df_3 = getMatchProportion(train_df_3)\n",
    "print(train_df_3[['match_category_proportion', 'match_property_proportion']].head(10))\n",
    "\n",
    "train_df_2 = getMatchProportion(train_df_2)\n",
    "train_df_1 = getMatchProportion(train_df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predict_category_number  predict_property_number  is_trade\n",
      "0                        5                        3         0\n",
      "1                        3                       11         0\n",
      "2                        2                        5         0\n",
      "3                        2                        2         0\n",
      "4                        3                        5         0\n",
      "5                        2                       11         0\n",
      "6                        8                        1         0\n",
      "7                        4                        8         0\n",
      "8                        4                        1         0\n",
      "9                        4                        1         0\n"
     ]
    }
   ],
   "source": [
    "#构造跟预测数目相关的特征\n",
    "def getPredictNumber(df):\n",
    "    df['predict_category_number'] = df['predict_category_set'].map(lambda x: len(x))\n",
    "    df['predict_property_number'] = df['predict_property_list'].map(lambda x: len(x))\n",
    "    return df\n",
    "\n",
    "train_df_3 = getPredictNumber(train_df_3)\n",
    "print(train_df_3[['predict_category_number', 'predict_property_number', 'is_trade']].head(10))\n",
    "\n",
    "train_df_2 = getPredictNumber(train_df_2)\n",
    "train_df_1 = getPredictNumber(train_df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   isFirstCategoryIn  isLastCategoryIn  is_trade\n",
      "0                  1                 0         0\n",
      "1                  0                 0         0\n",
      "2                  1                 1         0\n",
      "3                  1                 1         0\n",
      "4                  1                 1         0\n",
      "5                  1                 1         0\n",
      "6                  1                 0         0\n",
      "7                  1                 1         0\n",
      "8                  0                 1         0\n",
      "9                  1                 1         0\n"
     ]
    }
   ],
   "source": [
    "#构造跟类目预测精确性相关的特征\n",
    "def getPredictAccuracy(df):\n",
    "    isFirstCategoryIn = []\n",
    "    isLastCategoryIn = []\n",
    "    for x,y in df[['real_item_category_list', 'predict_category_list']].values:\n",
    "        if y[0] in x:\n",
    "            isFirstCategoryIn.append(1)\n",
    "        else:\n",
    "            isFirstCategoryIn.append(0)\n",
    "        if y[len(y)-1] in x:\n",
    "            isLastCategoryIn.append(1)\n",
    "        else:\n",
    "            isLastCategoryIn.append(0)\n",
    "    df['isFirstCategoryIn'] = isFirstCategoryIn\n",
    "    df['isLastCategoryIn'] = isLastCategoryIn\n",
    "    return df\n",
    "\n",
    "train_df_3 = getPredictAccuracy(train_df_3)\n",
    "print(train_df_3[['isFirstCategoryIn', 'isLastCategoryIn', 'is_trade']].head(10))\n",
    "\n",
    "train_df_2 = getPredictAccuracy(train_df_2)\n",
    "train_df_1 = getPredictAccuracy(train_df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  item_property_list  property_number  \\\n",
      "0  {6241534230954727302, 2072967855524022579, 510...               18   \n",
      "1  {6241534230954727302, 8399888477341455089, 163...               25   \n",
      "2  {6241534230954727302, 3257385717871127564, 282...               27   \n",
      "3  {6241534230954727302, 3163265386149801264, 203...               24   \n",
      "4  {6241534230954727302, 2072967855524022579, 487...               18   \n",
      "5  {2072967855524022579, 3163265386149801264, 839...               22   \n",
      "6  {6177034772098256046, 2072967855524022579, 829...               25   \n",
      "7  {6241534230954727302, 8399888477341455089, 714...               30   \n",
      "8  {3163265386149801264, 2032595807574745346, 248...               31   \n",
      "9  {6241534230954727302, 7138596973623795131, 124...               19   \n",
      "\n",
      "   category_number  \n",
      "0                3  \n",
      "1                3  \n",
      "2                2  \n",
      "3                3  \n",
      "4                2  \n",
      "5                2  \n",
      "6                2  \n",
      "7                2  \n",
      "8                3  \n",
      "9                2  \n"
     ]
    }
   ],
   "source": [
    "#添加商品属性个数以及类目个数特征\n",
    "def getCPNumber(df):\n",
    "    df['category_number'] = df['item_category_list'].map(lambda x: len(x))\n",
    "    df['property_number'] = df['item_property_list'].map(lambda x: len(x))\n",
    "    return df\n",
    "\n",
    "train_df_3 = getCPNumber(train_df_3)\n",
    "print(train_df_3[['item_property_list', 'property_number', 'category_number']].head(10))\n",
    "\n",
    "train_df_2 = getCPNumber(train_df_2)\n",
    "train_df_1 = getCPNumber(train_df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造跟商品实际的根类目和叶子类目特征\n",
    "train_df_1['real_first_category'] = train_df_1['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x.split(';')[0])\n",
    "train_df_1['real_last_category'] = train_df_1['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x[len(x.split(';')) -1])\n",
    "train_df_2['real_first_category'] = train_df_2['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x.split(';')[0])\n",
    "train_df_2['real_last_category'] = train_df_2['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x[len(x.split(';')) -1])\n",
    "train_df_3['real_first_category'] = train_df_3['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x.split(';')[0])\n",
    "train_df_3['real_last_category'] = train_df_3['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x[len(x.split(';')) -1])\n",
    "test_df['real_first_category'] = test_df['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x.split(';')[0])\n",
    "test_df['real_last_category'] = test_df['item_category_list'].map(lambda x: np.nan if len(x.split(';')) < 1 else x[len(x.split(';')) -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入测试集进行数据处理\n",
    "test_df = pd.read_csv('~/yuna/alimama/data/round2_ijcai_18_test_a_20180425.txt', sep=' ')\n",
    "\n",
    "test_df['date'] = test_df.context_timestamp.map(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "test_df['weekday'] = test_df['date'].map(lambda x: x.weekday())\n",
    "test_df['day'] = test_df['date'].map(lambda x: x.day)\n",
    "test_df['hour'] = test_df['date'].map(lambda x: x.hour)\n",
    "\n",
    "train_df_1['date'] = pd.to_datetime(train_df_1['date'])\n",
    "train_df_2['date'] = pd.to_datetime(train_df_2['date'])\n",
    "train_df_3['date'] = pd.to_datetime(train_df_3['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519888 entries, 0 to 519887\n",
      "Data columns (total 42 columns):\n",
      "instance_id                  519888 non-null int64\n",
      "item_id                      519888 non-null int64\n",
      "item_category_list           519888 non-null object\n",
      "item_property_list           519888 non-null object\n",
      "item_brand_id                519888 non-null int64\n",
      "item_city_id                 519888 non-null int64\n",
      "item_price_level             519888 non-null int64\n",
      "item_sales_level             519888 non-null int64\n",
      "item_collected_level         519888 non-null int64\n",
      "item_pv_level                519888 non-null int64\n",
      "user_id                      519888 non-null int64\n",
      "user_gender_id               519888 non-null int64\n",
      "user_age_level               519888 non-null int64\n",
      "user_occupation_id           519888 non-null int64\n",
      "user_star_level              519888 non-null int64\n",
      "context_id                   519888 non-null int64\n",
      "context_timestamp            519888 non-null int64\n",
      "context_page_id              519888 non-null int64\n",
      "predict_category_property    519888 non-null object\n",
      "shop_id                      519888 non-null int64\n",
      "shop_review_num_level        519888 non-null int64\n",
      "shop_review_positive_rate    519888 non-null float64\n",
      "shop_star_level              519888 non-null int64\n",
      "shop_score_service           519888 non-null float64\n",
      "shop_score_delivery          519888 non-null float64\n",
      "shop_score_description       519888 non-null float64\n",
      "date                         519888 non-null datetime64[ns]\n",
      "weekday                      519888 non-null int64\n",
      "day                          519888 non-null int64\n",
      "hour                         519888 non-null int64\n",
      "predict_category_list        519888 non-null object\n",
      "predict_category_set         519888 non-null object\n",
      "real_item_category_list      519888 non-null object\n",
      "predict_property_list        519888 non-null object\n",
      "match_category_proportion    519888 non-null float64\n",
      "match_property_proportion    519888 non-null float64\n",
      "predict_category_number      519888 non-null int64\n",
      "predict_property_number      519888 non-null int64\n",
      "isFirstCategoryIn            519888 non-null int64\n",
      "isLastCategoryIn             519888 non-null int64\n",
      "category_number              519888 non-null int64\n",
      "property_number              519888 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(6), int64(28), object(7)\n",
      "memory usage: 166.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = splitMultiFea(test_df)\n",
    "\n",
    "test_df = addContextFea(test_df)\n",
    "\n",
    "test_df = getCategoryFuture(test_df)\n",
    "\n",
    "test_df = getMatchProportion(test_df)\n",
    "\n",
    "test_df = getPredictNumber(test_df)\n",
    "\n",
    "test_df = getPredictAccuracy(test_df)\n",
    "\n",
    "test_df = getCPNumber(test_df)\n",
    "\n",
    "print(test_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origion_rate_mean :  0.011817072371250303\n",
      "origion_rate_var :  0.0013830353301735442\n",
      "alpha :  0.08795841609459074\n",
      "beta :  7.35537554440347\n",
      "origion_rate_mean :  0.012157241069404339\n",
      "origion_rate_var :  0.0015648190882514943\n",
      "alpha :  0.08114535818625888\n",
      "beta :  6.593507033997877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origion_rate_mean :  0.008908726008736599\n",
      "origion_rate_var :  0.0022100501382093032\n",
      "alpha :  0.026682482146988\n",
      "beta :  2.9684126774550754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "def getBayesSmoothParam(origion_rate):\n",
    "    origion_rate_mean = origion_rate.mean()\n",
    "    origion_rate_var = origion_rate.var()\n",
    "    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\n",
    "    print('origion_rate_mean : ', origion_rate_mean)\n",
    "    print('origion_rate_var : ', origion_rate_var)\n",
    "    print('alpha : ', alpha)\n",
    "    print('beta : ', beta)\n",
    "    return alpha, beta\n",
    "\n",
    "# 缩放字段至0-1\n",
    "def scalerFea(df, cols):\n",
    "    scaler = MinMaxScaler()\n",
    "    df[cols] = scaler.fit_transform(df[cols].values)\n",
    "    return df,scaler\n",
    "\n",
    "train_df_normal = pd.concat([train_df_1, train_df_2])\n",
    "\n",
    "def getHistoryInfoByCol(train_df_normal, train_df_1, train_df_2, train_df_3, test_df, colName):\n",
    "    train_df_pivot_table_all = pd.pivot_table(train_df_normal[['instance_id', colName]], index=[colName], values=['instance_id'], aggfunc=len)\n",
    "    train_df_pivot_table_all.reset_index(inplace=True)\n",
    "    train_df_pivot_table_all.rename(columns={'instance_id' : 'all_' + colName + '_click_number'}, inplace=True)\n",
    "#     print(train_df_pivot_table_all.head(10))\n",
    "\n",
    "    train_df_pivot_table_buy = pd.pivot_table(train_df_normal[['instance_id', colName]][train_df_normal.is_trade == 1], index=[colName], values=['instance_id'], aggfunc=len)\n",
    "    train_df_pivot_table_buy.reset_index(inplace=True)\n",
    "    train_df_pivot_table_buy.rename(columns={'instance_id' : 'all_' + colName + '_buy_number'}, inplace=True)\n",
    "#     print(train_df_pivot_table_buy.head(10))\n",
    "\n",
    "    train_df_pivot_table = pd.merge(train_df_pivot_table_all, train_df_pivot_table_buy, on=[colName], how='left')\n",
    "    train_df_pivot_table['all_' + colName + '_buy_number'] = train_df_pivot_table['all_' + colName + '_buy_number'].fillna(0)\n",
    "    train_df_pivot_table['all_' + colName + '_buy_number'][train_df_pivot_table[colName] == -1] = train_df_pivot_table['all_' + colName + '_buy_number'][train_df_pivot_table[colName] == -1] / len(train_df_pivot_table)\n",
    "    train_df_pivot_table['all_' + colName + '_click_number'][train_df_pivot_table[colName] == -1] = train_df_pivot_table['all_' + colName + '_click_number'][train_df_pivot_table[colName] == -1] / len(train_df_pivot_table)\n",
    "#     print(train_df_pivot_table['all_' + colName + '_click_number'][train_df_pivot_table.item_brand_id == -1])\n",
    "    train_df_pivot_table['history_' + colName + '_rate'] = train_df_pivot_table['all_' + colName + '_buy_number'] / train_df_pivot_table['all_' + colName + '_click_number']\n",
    "    alpha, beta = getBayesSmoothParam(train_df_pivot_table['history_' + colName + '_rate'])\n",
    "    train_df_pivot_table['history_' + colName + '_smooth_rate'] = (train_df_pivot_table['all_' + colName + '_buy_number'] + alpha) / (train_df_pivot_table['all_' + colName + '_click_number'] + alpha + beta)\n",
    "\n",
    "    train_df_pivot_table, all_buy_number_scaler = scalerFea(train_df_pivot_table, 'all_' + colName + '_buy_number')\n",
    "    train_df_pivot_table, all_click_number_scaler = scalerFea(train_df_pivot_table, 'all_' + colName + '_click_number')\n",
    "#     print(train_df_pivot_table.head(10))\n",
    "#     print(train_df_pivot_table.columns.values)\n",
    "\n",
    "    train_df_1 = pd.merge(train_df_1, train_df_pivot_table, on=[colName], how='left')\n",
    "    train_df_2 = pd.merge(train_df_2, train_df_pivot_table, on=[colName], how='left')\n",
    "    train_df_3 = pd.merge(train_df_3, train_df_pivot_table, on=[colName], how='left')\n",
    "    train_df_3['all_' + colName + '_click_number'] = train_df_3['all_' + colName + '_click_number'].fillna(0)\n",
    "    train_df_3['all_' + colName + '_buy_number'] = train_df_3['all_' + colName + '_buy_number'].fillna(0)\n",
    "    train_df_3['history_' + colName + '_smooth_rate'] = train_df_3['history_' + colName + '_smooth_rate'].fillna((alpha / (alpha + beta)))\n",
    "\n",
    "    test_df = pd.merge(test_df, train_df_pivot_table, on=[colName], how='left')\n",
    "    test_df['all_' + colName + '_click_number'] = test_df['all_' + colName + '_click_number'].fillna(0)\n",
    "    test_df['all_' + colName + '_buy_number'] = test_df['all_' + colName + '_buy_number'].fillna(0)\n",
    "    test_df['history_' + colName + '_smooth_rate'] = test_df['history_' + colName + '_smooth_rate'].fillna((alpha / (alpha + beta)))\n",
    "    \n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getHistoryInfoByCol(train_df_normal, train_df_1, train_df_2, train_df_3, test_df, 'item_brand_id')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getHistoryInfoByCol(train_df_normal, train_df_1, train_df_2, train_df_3, test_df, 'shop_id')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getHistoryInfoByCol(train_df_normal, train_df_1, train_df_2, train_df_3, test_df, 'item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计过去一个小时某用户点击某个相同商品的次数\n",
    "def getOneHourSameItemCount(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_id_str'] = train_df_1_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_id_str'] = train_df_2_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_id_str'] = train_df_3_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_id_str'] = test_df_copy['item_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_id'] = test_df_copy['user_id_str'] + test_df_copy['item_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_id','date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_item'] = tempDf['user_item_id'].shift(1)\n",
    "    tempDf['last_user_item'] = tempDf['last_user_item']==tempDf['user_item_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item, 'last_show_time'] = np.nan\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['date'] - tempDf['last_show_time']\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['ui_last_show_timedelta'].dt.seconds\n",
    "    tempDf['ui_last_show_timedelta'].fillna(999999, inplace=True)\n",
    "    hourShowList = []\n",
    "    hourShowTemp = {}\n",
    "    for same, dt, show in tempDf[['last_user_item','date','show']].values:\n",
    "        if same:\n",
    "            [hourShowTemp.pop(k) for k in list(hourShowTemp) if k<dt-timedelta(hours=1)]\n",
    "            hourShowList.append(np.sum(list(hourShowTemp.values())))\n",
    "            hourShowTemp[dt] = show\n",
    "        else:\n",
    "            hourShowList.append(0)\n",
    "            hourShowTemp = {dt:show}\n",
    "    tempDf['lastOneHour_sameItem_count'] = hourShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_id', 'date', 'lastOneHour_sameItem_count']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_1['lastOneHour_sameItem_count'] = train_df_1_copy['lastOneHour_sameItem_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_id', 'date', 'lastOneHour_sameItem_count']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_2['lastOneHour_sameItem_count'] = train_df_2_copy['lastOneHour_sameItem_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_id', 'date', 'lastOneHour_sameItem_count']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_3['lastOneHour_sameItem_count'] = train_df_3_copy['lastOneHour_sameItem_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_id', 'date', 'lastOneHour_sameItem_count']], on = ['user_item_id', 'date'], how='left')\n",
    "    test_df['lastOneHour_sameItem_count'] = test_df_copy['lastOneHour_sameItem_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getOneHourSameItemCount(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计过去一个小时某用户点击同种根类目商品的次数\n",
    "def getOneHourSameFirstCategoryCount(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_first_category_str'] = train_df_1_copy['real_first_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_first_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_first_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_first_category_str'] = train_df_2_copy['real_first_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_first_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_first_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_first_category_str'] = train_df_3_copy['real_first_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_first_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_first_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_first_category_str'] = test_df_copy['real_first_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_first_category'] = test_df_copy['user_id_str'] + test_df_copy['real_first_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_first_category','date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_real_first_category'] = tempDf['user_real_first_category'].shift(1)\n",
    "    tempDf['last_user_real_first_category'] = tempDf['last_user_real_first_category']==tempDf['user_real_first_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_first_category, 'last_show_time'] = np.nan\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['date'] - tempDf['last_show_time']\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['ui_last_show_timedelta'].dt.seconds\n",
    "    tempDf['ui_last_show_timedelta'].fillna(999999, inplace=True)\n",
    "    hourShowList = []\n",
    "    hourShowTemp = {}\n",
    "    for same, dt, show in tempDf[['last_user_real_first_category','date','show']].values:\n",
    "        if same:\n",
    "            [hourShowTemp.pop(k) for k in list(hourShowTemp) if k<dt-timedelta(hours=1)]\n",
    "            hourShowList.append(np.sum(list(hourShowTemp.values())))\n",
    "            hourShowTemp[dt] = show\n",
    "        else:\n",
    "            hourShowList.append(0)\n",
    "            hourShowTemp = {dt:show}\n",
    "    tempDf['lastOneHour_sameFirstCategory_count'] = hourShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_first_category', 'date', 'lastOneHour_sameFirstCategory_count']], how='left', on=['user_real_first_category', 'date'])\n",
    "    train_df_1['lastOneHour_sameFirstCategory_count'] = train_df_1_copy['lastOneHour_sameFirstCategory_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_first_category', 'date', 'lastOneHour_sameFirstCategory_count']], how='left', on=['user_real_first_category', 'date'])\n",
    "    train_df_2['lastOneHour_sameFirstCategory_count'] = train_df_2_copy['lastOneHour_sameFirstCategory_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_first_category', 'date', 'lastOneHour_sameFirstCategory_count']], how='left', on=['user_real_first_category', 'date'])\n",
    "    train_df_3['lastOneHour_sameFirstCategory_count'] = train_df_3_copy['lastOneHour_sameFirstCategory_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_first_category', 'date', 'lastOneHour_sameFirstCategory_count']], on = ['user_real_first_category', 'date'], how='left')\n",
    "    test_df['lastOneHour_sameFirstCategory_count'] = test_df_copy['lastOneHour_sameFirstCategory_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getOneHourSameFirstCategoryCount(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instance_id' 'item_id' 'item_category_list' 'item_property_list'\n",
      " 'item_brand_id' 'item_city_id' 'item_price_level' 'item_sales_level'\n",
      " 'item_collected_level' 'item_pv_level' 'user_id' 'user_gender_id'\n",
      " 'user_age_level' 'user_occupation_id' 'user_star_level' 'context_id'\n",
      " 'context_timestamp' 'context_page_id' 'predict_category_property'\n",
      " 'shop_id' 'shop_review_num_level' 'shop_review_positive_rate'\n",
      " 'shop_star_level' 'shop_score_service' 'shop_score_delivery'\n",
      " 'shop_score_description' 'is_trade' 'date' 'weekday' 'day' 'hour'\n",
      " 'predict_category_list' 'predict_category_set' 'real_item_category_list'\n",
      " 'predict_property_list' 'match_category_proportion'\n",
      " 'match_property_proportion' 'predict_category_number'\n",
      " 'predict_property_number' 'isFirstCategoryIn' 'isLastCategoryIn'\n",
      " 'category_number' 'property_number' 'all_item_brand_id_click_number'\n",
      " 'all_item_brand_id_buy_number' 'history_item_brand_id_rate'\n",
      " 'history_item_brand_id_smooth_rate' 'all_shop_id_click_number'\n",
      " 'all_shop_id_buy_number' 'history_shop_id_rate'\n",
      " 'history_shop_id_smooth_rate' 'all_item_id_click_number'\n",
      " 'all_item_id_buy_number' 'history_item_id_rate'\n",
      " 'history_item_id_smooth_rate' 'lastOneHour_sameItem_count'\n",
      " 'real_first_category' 'real_last_category'\n",
      " 'lastOneHour_sameFirstCategory_count']\n",
      "['instance_id' 'item_id' 'item_category_list' 'item_property_list'\n",
      " 'item_brand_id' 'item_city_id' 'item_price_level' 'item_sales_level'\n",
      " 'item_collected_level' 'item_pv_level' 'user_id' 'user_gender_id'\n",
      " 'user_age_level' 'user_occupation_id' 'user_star_level' 'context_id'\n",
      " 'context_timestamp' 'context_page_id' 'predict_category_property'\n",
      " 'shop_id' 'shop_review_num_level' 'shop_review_positive_rate'\n",
      " 'shop_star_level' 'shop_score_service' 'shop_score_delivery'\n",
      " 'shop_score_description' 'date' 'weekday' 'day' 'hour'\n",
      " 'predict_category_list' 'predict_category_set' 'real_item_category_list'\n",
      " 'predict_property_list' 'match_category_proportion'\n",
      " 'match_property_proportion' 'predict_category_number'\n",
      " 'predict_property_number' 'isFirstCategoryIn' 'isLastCategoryIn'\n",
      " 'category_number' 'property_number' 'all_item_brand_id_click_number'\n",
      " 'all_item_brand_id_buy_number' 'history_item_brand_id_rate'\n",
      " 'history_item_brand_id_smooth_rate' 'all_shop_id_click_number'\n",
      " 'all_shop_id_buy_number' 'history_shop_id_rate'\n",
      " 'history_shop_id_smooth_rate' 'all_item_id_click_number'\n",
      " 'all_item_id_buy_number' 'history_item_id_rate'\n",
      " 'history_item_id_smooth_rate' 'lastOneHour_sameItem_count'\n",
      " 'real_first_category' 'real_last_category'\n",
      " 'lastOneHour_sameFirstCategory_count']\n"
     ]
    }
   ],
   "source": [
    "print(train_df_1.columns.values)\n",
    "print(test_df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "#删除某些不必要的列\n",
    "drop_fea = ['item_category_list', 'item_property_list', 'predict_category_property', \n",
    "            'predict_category_list', 'predict_category_set', 'real_item_category_list', \n",
    "            'predict_property_list']\n",
    "\n",
    "print(len(train_df_1.columns.values))\n",
    "train_df_1.drop(drop_fea, axis=1, inplace=True)\n",
    "print(len(train_df_1.columns.values))\n",
    "\n",
    "train_df_2.drop(drop_fea, axis=1, inplace=True)\n",
    "train_df_3.drop(drop_fea, axis=1, inplace=True)\n",
    "test_df.drop(drop_fea, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计过去一个小时某用户点击同种叶子类目商品的次数\n",
    "def getOneHourSameLastCategoryCount(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_last_category_str'] = train_df_1_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_last_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_last_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_last_category_str'] = train_df_2_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_last_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_last_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_last_category_str'] = train_df_3_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_last_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_last_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_last_category_str'] = test_df_copy['real_last_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_last_category'] = test_df_copy['user_id_str'] + test_df_copy['real_last_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_last_category','date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['user_real_last_category'].shift(1)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['last_user_real_last_category']==tempDf['user_real_last_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_last_category, 'last_show_time'] = np.nan\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['date'] - tempDf['last_show_time']\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['ui_last_show_timedelta'].dt.seconds\n",
    "    tempDf['ui_last_show_timedelta'].fillna(999999, inplace=True)\n",
    "    hourShowList = []\n",
    "    hourShowTemp = {}\n",
    "    for same, dt, show in tempDf[['last_user_real_last_category','date','show']].values:\n",
    "        if same:\n",
    "            [hourShowTemp.pop(k) for k in list(hourShowTemp) if k<dt-timedelta(hours=1)]\n",
    "            hourShowList.append(np.sum(list(hourShowTemp.values())))\n",
    "            hourShowTemp[dt] = show\n",
    "        else:\n",
    "            hourShowList.append(0)\n",
    "            hourShowTemp = {dt:show}\n",
    "    tempDf['lastOneHour_sameLastCategory_count'] = hourShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_last_category', 'date', 'lastOneHour_sameLastCategory_count']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_1['lastOneHour_sameLastCategory_count'] = train_df_1_copy['lastOneHour_sameLastCategory_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_last_category', 'date', 'lastOneHour_sameLastCategory_count']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_2['lastOneHour_sameLastCategory_count'] = train_df_2_copy['lastOneHour_sameLastCategory_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_last_category', 'date', 'lastOneHour_sameLastCategory_count']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_3['lastOneHour_sameLastCategory_count'] = train_df_3_copy['lastOneHour_sameLastCategory_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_last_category', 'date', 'lastOneHour_sameLastCategory_count']], on = ['user_real_last_category', 'date'], how='left')\n",
    "    test_df['lastOneHour_sameLastCategory_count'] = test_df_copy['lastOneHour_sameLastCategory_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getOneHourSameLastCategoryCount(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计过去一个小时某用户点击同种品牌商品的次数\n",
    "def getOneHourSameBrandCount(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['brand_id_str'] = train_df_1_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_brand_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['brand_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['brand_id_str'] = train_df_2_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_brand_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['brand_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['brand_id_str'] = train_df_3_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_brand_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['brand_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['brand_id_str'] = test_df_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_brand_id'] = test_df_copy['user_id_str'] + test_df_copy['brand_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_brand_id','date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_brand_id'] = tempDf['user_brand_id'].shift(1)\n",
    "    tempDf['last_user_brand_id'] = tempDf['last_user_brand_id']==tempDf['user_brand_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_brand_id, 'last_show_time'] = np.nan\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['date'] - tempDf['last_show_time']\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['ui_last_show_timedelta'].dt.seconds\n",
    "    tempDf['ui_last_show_timedelta'].fillna(999999, inplace=True)\n",
    "    hourShowList = []\n",
    "    hourShowTemp = {}\n",
    "    for same, dt, show in tempDf[['last_user_brand_id','date','show']].values:\n",
    "        if same:\n",
    "            [hourShowTemp.pop(k) for k in list(hourShowTemp) if k<dt-timedelta(hours=1)]\n",
    "            hourShowList.append(np.sum(list(hourShowTemp.values())))\n",
    "            hourShowTemp[dt] = show\n",
    "        else:\n",
    "            hourShowList.append(0)\n",
    "            hourShowTemp = {dt:show}\n",
    "    tempDf['lastOneHour_sameBrand_count'] = hourShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_brand_id', 'date', 'lastOneHour_sameBrand_count']], how='left', on=['user_brand_id', 'date'])\n",
    "    train_df_1['lastOneHour_sameBrand_count'] = train_df_1_copy['lastOneHour_sameBrand_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_brand_id', 'date', 'lastOneHour_sameBrand_count']], how='left', on=['user_brand_id', 'date'])\n",
    "    train_df_2['lastOneHour_sameBrand_count'] = train_df_2_copy['lastOneHour_sameBrand_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_brand_id', 'date', 'lastOneHour_sameBrand_count']], how='left', on=['user_brand_id', 'date'])\n",
    "    train_df_3['lastOneHour_sameBrand_count'] = train_df_3_copy['lastOneHour_sameBrand_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_brand_id', 'date', 'lastOneHour_sameBrand_count']], on = ['user_brand_id', 'date'], how='left')\n",
    "    test_df['lastOneHour_sameBrand_count'] = test_df_copy['lastOneHour_sameBrand_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getOneHourSameBrandCount(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计过去一个小时某用户点击同种店铺商品的次数\n",
    "def getOneHourSameShopCount(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_shop_id_str'] = train_df_1_copy['shop_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_shop_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_shop_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_shop_id_str'] = train_df_2_copy['shop_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_shop_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_shop_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_shop_id_str'] = train_df_3_copy['shop_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_shop_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_shop_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_shop_id_str'] = test_df_copy['shop_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_shop_id'] = test_df_copy['user_id_str'] + test_df_copy['item_shop_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_shop_id','date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_item_shop_id'] = tempDf['user_item_shop_id'].shift(1)\n",
    "    tempDf['last_user_item_shop_id'] = tempDf['last_user_item_shop_id']==tempDf['user_item_shop_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_shop_id, 'last_show_time'] = np.nan\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['date'] - tempDf['last_show_time']\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['ui_last_show_timedelta'].dt.seconds\n",
    "    tempDf['ui_last_show_timedelta'].fillna(999999, inplace=True)\n",
    "    hourShowList = []\n",
    "    hourShowTemp = {}\n",
    "    for same, dt, show in tempDf[['last_user_item_shop_id','date','show']].values:\n",
    "        if same:\n",
    "            [hourShowTemp.pop(k) for k in list(hourShowTemp) if k<dt-timedelta(hours=1)]\n",
    "            hourShowList.append(np.sum(list(hourShowTemp.values())))\n",
    "            hourShowTemp[dt] = show\n",
    "        else:\n",
    "            hourShowList.append(0)\n",
    "            hourShowTemp = {dt:show}\n",
    "    tempDf['lastOneHour_sameShop_count'] = hourShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_shop_id', 'date', 'lastOneHour_sameShop_count']], how='left', on=['user_item_shop_id', 'date'])\n",
    "    train_df_1['lastOneHour_sameShop_count'] = train_df_1_copy['lastOneHour_sameShop_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_shop_id', 'date', 'lastOneHour_sameShop_count']], how='left', on=['user_item_shop_id', 'date'])\n",
    "    train_df_2['lastOneHour_sameShop_count'] = train_df_2_copy['lastOneHour_sameShop_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_shop_id', 'date', 'lastOneHour_sameShop_count']], how='left', on=['user_item_shop_id', 'date'])\n",
    "    train_df_3['lastOneHour_sameShop_count'] = train_df_3_copy['lastOneHour_sameShop_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_shop_id', 'date', 'lastOneHour_sameShop_count']], on = ['user_item_shop_id', 'date'], how='left')\n",
    "    test_df['lastOneHour_sameShop_count'] = test_df_copy['lastOneHour_sameShop_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getOneHourSameShopCount(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取是否是该用户在这1个小时内第一次点击这个商品的特征\n",
    "def getIsOneHourFirstClickItem(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_id_str'] = train_df_1_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_id_str'] = train_df_2_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_id_str'] = train_df_3_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_id_str'] = test_df_copy['item_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_id'] = test_df_copy['user_id_str'] + test_df_copy['item_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_id','date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_item_id'] = tempDf['user_item_id'].shift(1)\n",
    "    tempDf['last_user_item_id'] = tempDf['last_user_item_id']==tempDf['user_item_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_id, 'last_show_time'] = np.nan\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['date'] - tempDf['last_show_time']\n",
    "    tempDf['ui_last_show_timedelta'] = tempDf['ui_last_show_timedelta'].dt.seconds\n",
    "    tempDf['ui_last_show_timedelta'].fillna(999999, inplace=True)\n",
    "    hourShowList = []\n",
    "    hourShowTemp = {}\n",
    "    for same, dt, show in tempDf[['last_user_item_id','date','show']].values:\n",
    "        if same:\n",
    "            [hourShowTemp.pop(k) for k in list(hourShowTemp) if k<dt-timedelta(hours=1)]\n",
    "            if len(hourShowTemp) > 0:\n",
    "                hourShowList.append(0)\n",
    "            else:\n",
    "                hourShowList.append(1)\n",
    "            hourShowTemp[dt] = show\n",
    "        else:\n",
    "            hourShowList.append(1)\n",
    "            hourShowTemp = {dt:show}\n",
    "    tempDf['isLastOneHour_firstClickItem'] = hourShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_id', 'date', 'isLastOneHour_firstClickItem']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_1['isLastOneHour_firstClickItem'] = train_df_1_copy['isLastOneHour_firstClickItem']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_id', 'date', 'isLastOneHour_firstClickItem']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_2['isLastOneHour_firstClickItem'] = train_df_2_copy['isLastOneHour_firstClickItem']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_id', 'date', 'isLastOneHour_firstClickItem']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_3['isLastOneHour_firstClickItem'] = train_df_3_copy['isLastOneHour_firstClickItem']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_id', 'date', 'isLastOneHour_firstClickItem']], on = ['user_item_id', 'date'], how='left')\n",
    "    test_df['isLastOneHour_firstClickItem'] = test_df_copy['isLastOneHour_firstClickItem']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getIsOneHourFirstClickItem(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    }
   ],
   "source": [
    "# 统计某用户距离上次点击相同商品的时间\n",
    "def getUserItemLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    \n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_id_str'] = train_df_1_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_id_str'] = train_df_2_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_id_str'] = train_df_3_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_id_str'] = test_df_copy['item_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_id'] = test_df_copy['user_id_str'] + test_df_copy['item_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    temp_df['user_id_str'] = temp_df['user_id'].map(lambda x: str(x))\n",
    "    temp_df['item_id_str'] = temp_df['item_id'].map(lambda x: str(x))\n",
    "    temp_df['user_item_id'] = temp_df['user_id_str'] + temp_df['item_id_str']\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_id', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_item_id'] = tempDf['user_item_id'].shift(1)\n",
    "    tempDf['last_user_item_id'] = tempDf['last_user_item_id']==tempDf['user_item_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_id, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_item_id', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['userItem_lastClickDeltaTime'] = historyShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_id', 'date', 'userItem_lastClickDeltaTime']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_1['userItem_lastClickDeltaTime'] = train_df_1_copy['userItem_lastClickDeltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_id', 'date', 'userItem_lastClickDeltaTime']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_2['userItem_lastClickDeltaTime'] = train_df_2_copy['userItem_lastClickDeltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_id', 'date', 'userItem_lastClickDeltaTime']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_3['userItem_lastClickDeltaTime'] = train_df_3_copy['userItem_lastClickDeltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_id', 'date', 'userItem_lastClickDeltaTime']], on = ['user_item_id', 'date'], how='left')\n",
    "    test_df['userItem_lastClickDeltaTime'] = test_df_copy['userItem_lastClickDeltaTime']\n",
    "    print(len(train_df_1))\n",
    "    \n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserItemLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计某用户距离上次点击相同品牌商品的时间\n",
    "def getUserBrandLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_brand_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['brand_id_str'] = train_df_1_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_brand_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['brand_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['brand_id_str'] = train_df_2_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_brand_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['brand_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['brand_id_str'] = train_df_3_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_brand_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['brand_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['brand_id_str'] = test_df_copy['item_brand_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_brand_id'] = test_df_copy['user_id_str'] + test_df_copy['brand_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_brand_id', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_item_brand_id'] = tempDf['user_brand_id'].shift(1)\n",
    "    tempDf['last_user_item_brand_id'] = tempDf['last_user_item_brand_id']==tempDf['user_brand_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_brand_id, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_item_brand_id', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['userBrand_lastClickDeltaTime'] = historyShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_brand_id', 'date', 'userBrand_lastClickDeltaTime']], how='left', on=['user_brand_id', 'date'])\n",
    "    train_df_1['userBrand_lastClickDeltaTime'] = train_df_1_copy['userBrand_lastClickDeltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_brand_id', 'date', 'userBrand_lastClickDeltaTime']], how='left', on=['user_brand_id', 'date'])\n",
    "    train_df_2['userBrand_lastClickDeltaTime'] = train_df_2_copy['userBrand_lastClickDeltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_brand_id', 'date', 'userBrand_lastClickDeltaTime']], how='left', on=['user_brand_id', 'date'])\n",
    "    train_df_3['userBrand_lastClickDeltaTime'] = train_df_3_copy['userBrand_lastClickDeltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_brand_id', 'date', 'userBrand_lastClickDeltaTime']], on = ['user_brand_id', 'date'], how='left')\n",
    "    test_df['userBrand_lastClickDeltaTime'] = test_df_copy['userBrand_lastClickDeltaTime']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserBrandLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计某用户距离上次点击相同店铺的时间\n",
    "def getUserShopLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'shop_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_shop_id_str'] = train_df_1_copy['shop_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_shop_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_shop_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_shop_id_str'] = train_df_2_copy['shop_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_shop_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_shop_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_shop_id_str'] = train_df_3_copy['shop_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_shop_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_shop_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_shop_id_str'] = test_df_copy['shop_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_shop_id'] = test_df_copy['user_id_str'] + test_df_copy['item_shop_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_shop_id', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_shop_id'] = tempDf['user_item_shop_id'].shift(1)\n",
    "    tempDf['last_user_shop_id'] = tempDf['last_user_shop_id']==tempDf['user_item_shop_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_shop_id, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_shop_id', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['userShop_lastClickDeltaTime'] = historyShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_shop_id', 'date', 'userShop_lastClickDeltaTime']], how='left', on=['user_item_shop_id', 'date'])\n",
    "    train_df_1['userShop_lastClickDeltaTime'] = train_df_1_copy['userShop_lastClickDeltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_shop_id', 'date', 'userShop_lastClickDeltaTime']], how='left', on=['user_item_shop_id', 'date'])\n",
    "    train_df_2['userShop_lastClickDeltaTime'] = train_df_2_copy['userShop_lastClickDeltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_shop_id', 'date', 'userShop_lastClickDeltaTime']], how='left', on=['user_item_shop_id', 'date'])\n",
    "    train_df_3['userShop_lastClickDeltaTime'] = train_df_3_copy['userShop_lastClickDeltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_shop_id', 'date', 'userShop_lastClickDeltaTime']], on = ['user_item_shop_id', 'date'], how='left')\n",
    "    test_df['userShop_lastClickDeltaTime'] = test_df_copy['userShop_lastClickDeltaTime']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserShopLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计某用户距离上次点击相同根类目的时间\n",
    "def getUserFirstCategoryLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_first_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_first_category_str'] = train_df_1_copy['real_first_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_first_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_first_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_first_category_str'] = train_df_2_copy['real_first_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_first_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_first_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_first_category_str'] = train_df_3_copy['real_first_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_first_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_first_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_first_category_str'] = test_df_copy['real_first_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_first_category'] = test_df_copy['user_id_str'] + test_df_copy['real_first_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_first_category', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_real_first_category'] = tempDf['user_real_first_category'].shift(1)\n",
    "    tempDf['last_user_real_first_category'] = tempDf['last_user_real_first_category']==tempDf['user_real_first_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_first_category, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_real_first_category', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['userFirstCategory_lastClickDeltaTime'] = historyShowList \n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_first_category', 'date', 'userFirstCategory_lastClickDeltaTime']], how='left', on=['user_real_first_category', 'date'])\n",
    "    train_df_1['userFirstCategory_lastClickDeltaTime'] = train_df_1_copy['userFirstCategory_lastClickDeltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_first_category', 'date', 'userFirstCategory_lastClickDeltaTime']], how='left', on=['user_real_first_category', 'date'])\n",
    "    train_df_2['userFirstCategory_lastClickDeltaTime'] = train_df_2_copy['userFirstCategory_lastClickDeltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_first_category', 'date', 'userFirstCategory_lastClickDeltaTime']], how='left', on=['user_real_first_category', 'date'])\n",
    "    train_df_3['userFirstCategory_lastClickDeltaTime'] = train_df_3_copy['userFirstCategory_lastClickDeltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_first_category', 'date', 'userFirstCategory_lastClickDeltaTime']], on = ['user_real_first_category', 'date'], how='left')\n",
    "    test_df['userFirstCategory_lastClickDeltaTime'] = test_df_copy['userFirstCategory_lastClickDeltaTime']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserFirstCategoryLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计某用户距离上次点击相同叶子类目的时间\n",
    "def getUserLastCategoryLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_last_category_str'] = train_df_1_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_last_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_last_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_last_category_str'] = train_df_2_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_last_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_last_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_last_category_str'] = train_df_3_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_last_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_last_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_last_category_str'] = test_df_copy['real_last_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_last_category'] = test_df_copy['user_id_str'] + test_df_copy['real_last_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_last_category', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['user_real_last_category'].shift(1)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['last_user_real_last_category']==tempDf['user_real_last_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_last_category, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_real_last_category', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['userLastCategory_lastClickDeltaTime'] = historyShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_last_category', 'date', 'userLastCategory_lastClickDeltaTime']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_1['userLastCategory_lastClickDeltaTime'] = train_df_1_copy['userLastCategory_lastClickDeltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_last_category', 'date', 'userLastCategory_lastClickDeltaTime']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_2['userLastCategory_lastClickDeltaTime'] = train_df_2_copy['userLastCategory_lastClickDeltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_last_category', 'date', 'userLastCategory_lastClickDeltaTime']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_3['userLastCategory_lastClickDeltaTime'] = train_df_3_copy['userLastCategory_lastClickDeltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_last_category', 'date', 'userLastCategory_lastClickDeltaTime']], on = ['user_real_last_category', 'date'], how='left')\n",
    "    test_df['userLastCategory_lastClickDeltaTime'] = test_df_copy['userLastCategory_lastClickDeltaTime']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserLastCategoryLastClickDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义添加每个小时转化率特征，test数据集采用预测方法填充\n",
    "def getHourTradeRate(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    train_df_all = pd.concat([train_df_1, train_df_2, train_df_3])\n",
    "    train_df_hour_pivot_table_all = pd.pivot_table(train_df_all[['day', 'hour', 'instance_id']], index=['day', 'hour'], values=['instance_id'], aggfunc=len)\n",
    "    train_df_hour_pivot_table_all.reset_index(inplace=True)\n",
    "    train_df_hour_pivot_table_all.rename(columns={'instance_id' : 'all_click_number'}, inplace=True)\n",
    "\n",
    "    train_df_hour_pivot_table_buy = pd.pivot_table(train_df_all[['day', 'hour', 'instance_id']][train_df_all.is_trade == 1], index=['day', 'hour'], values=['instance_id'], aggfunc=len)\n",
    "    train_df_hour_pivot_table_buy.reset_index(inplace=True)\n",
    "    train_df_hour_pivot_table_buy.rename(columns={'instance_id' : 'all_buy_number'}, inplace=True)\n",
    "\n",
    "    train_df_hour_pivot_table_all = pd.merge(train_df_hour_pivot_table_all, train_df_hour_pivot_table_buy, on=['day', 'hour'], how='left')\n",
    "    train_df_hour_pivot_table_all['rate'] = train_df_hour_pivot_table_all['all_buy_number'] / train_df_hour_pivot_table_all['all_click_number']\n",
    "\n",
    "    print(train_df_hour_pivot_table_all[train_df_hour_pivot_table_all.day == 7])\n",
    "\n",
    "    hour_11_normal_mean = train_df_hour_pivot_table_all['rate'][train_df_hour_pivot_table_all.hour == 11].mean()\n",
    "    hour_11_diff = train_df_hour_pivot_table_all['rate'][(train_df_hour_pivot_table_all.hour == 11) & (train_df_hour_pivot_table_all.day == 7)] - hour_11_normal_mean\n",
    "    print(hour_11_diff)\n",
    "\n",
    "    hour_nextHalf_normal_mean = pd.pivot_table(train_df_hour_pivot_table_all[['hour', 'rate']][(train_df_hour_pivot_table_all.hour > 11) & ((train_df_hour_pivot_table_all.day == 31) | (train_df_hour_pivot_table_all.day <= 4))], index=['hour'], values=['rate'], aggfunc=mean)\n",
    "    hour_nextHalf_normal_mean.reset_index(inplace=True)\n",
    "    hour_nextHalf_normal_mean['special_rate'] = hour_nextHalf_normal_mean['rate'] + hour_11_diff.values\n",
    "    print(hour_nextHalf_normal_mean.head)\n",
    "    print(hour_nextHalf_normal_mean['special_rate'].mean())\n",
    "    \n",
    "    train_df_hour_pivot_table_all.rename(columns={'rate' : 'hour_rate'}, inplace=True)\n",
    "    hour_nextHalf_normal_mean.rename(columns={'special_rate' : 'hour_rate'}, inplace=True)\n",
    "    \n",
    "    train_df_1 = pd.merge(train_df_1, train_df_hour_pivot_table_all[['day', 'hour', 'hour_rate']], on=['day', 'hour'], how='left')\n",
    "    train_df_2 = pd.merge(train_df_2, train_df_hour_pivot_table_all[['day', 'hour', 'hour_rate']], on=['day', 'hour'], how='left')\n",
    "    train_df_3 = pd.merge(train_df_3, train_df_hour_pivot_table_all[['day', 'hour', 'hour_rate']], on=['day', 'hour'], how='left')\n",
    "    test_df = pd.merge(test_df, hour_nextHalf_normal_mean[['hour', 'hour_rate']], on=['hour'], how='left')\n",
    "    print(train_df_3[['day', 'hour', 'hour_rate']].head(10))\n",
    "    print(test_df[['day', 'hour', 'hour_rate']].head(10))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getHourTradeRate(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义获取某种店铺，品牌，城市对应商品种类，用户数的函数\n",
    "def getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, colName1, colName2, newColName):\n",
    "\n",
    "    df = pd.concat([train_df_1[[colName1, colName2, 'instance_id']], train_df_2[[colName1, colName2, 'instance_id']], train_df_3[[colName1, colName2, 'instance_id']], test_df[[colName1, colName2, 'instance_id']]])\n",
    "    temp_df = df[[colName1, colName2, 'instance_id']]\n",
    "    tempDf = temp_df.sort_values(by=colName1, ascending=False)\n",
    "    tempDf['last_' + colName1] = tempDf[colName1].shift(1)\n",
    "    tempDf['same'] = tempDf['last_' + colName1]==tempDf[colName1]\n",
    "#     print(tempDf.head(10))\n",
    "    colName1List = []\n",
    "    countList = []\n",
    "    colName2Set = set()\n",
    "    for same, col2, last_col1 in tempDf[['same', colName2, 'last_' + colName1]].values:\n",
    "        if same:\n",
    "            colName2Set.add(col2)\n",
    "        else:\n",
    "            colName1List.append(last_col1)\n",
    "            countList.append(len(colName2Set))\n",
    "            colName2Set = {col2}\n",
    "    #处理最后一行数据\n",
    "    last_col1 = tempDf.iloc[-1][colName1]\n",
    "    last_count = len(colName2Set)\n",
    "    colName1List.append(last_col1)\n",
    "    countList.append(last_count)\n",
    "\n",
    "    #将结果组合到tempDf中\n",
    "    result_df = {colName1: colName1List, newColName: countList}\n",
    "    result_df = DataFrame(result_df)\n",
    "    result_df = result_df[1:]\n",
    "\n",
    "    tempDf = tempDf.drop_duplicates([colName1])\n",
    "    tempDf[newColName] = result_df[newColName].values\n",
    "\n",
    "    print(len(train_df_1))\n",
    "    train_df_1 = pd.merge(train_df_1, tempDf[[colName1, newColName]], on=[colName1], how='left')\n",
    "    print(len(train_df_1))\n",
    "    train_df_2 = pd.merge(train_df_2, tempDf[[colName1, newColName]], on=[colName1], how='left')\n",
    "    train_df_3 = pd.merge(train_df_3, tempDf[[colName1, newColName]], on=[colName1], how='left')\n",
    "    test_df = pd.merge(test_df, tempDf[[colName1, newColName]], on=[colName1], how='left')\n",
    "\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, 'shop_id', 'item_id', 'shop_item_classNumber')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, 'item_brand_id', 'item_id', 'brand_item_classNumber')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, 'item_city_id', 'item_id', 'city_item_classNumber')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, 'shop_id', 'user_id', 'shop_user_classNumber')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, 'item_brand_id', 'user_id', 'brand_user_classNumber')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getCorrespondNumber(train_df_1, train_df_2, train_df_3, test_df, 'item_city_id', 'user_id', 'city_user_classNumber')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义获取用户浏览过商品的平均值，众数，中位数，最大值，最小值\n",
    "def getUserItemStatFuture(train_df_1, train_df_2, train_df_3, test_df, colName):\n",
    "    df = pd.concat([train_df_1, train_df_2, train_df_3, test_df])\n",
    "    df_user_item_pivot_table = pd.pivot_table(df, index=['user_id'], values=[colName], aggfunc=[np.mean, np.max, np.min, np.median])\n",
    "    df_user_item_pivot_table.reset_index(inplace=True)\n",
    "    df_user_item_pivot_table.columns = ['user_id', colName + '_mean', colName + '_max', colName + '_min', colName + '_median']\n",
    "\n",
    "    df_mode_pivot_table = pd.pivot_table(df[['user_id', colName, 'instance_id']], index=['user_id', colName], values=['instance_id'], aggfunc=len)\n",
    "    df_mode_pivot_table.reset_index(inplace=True)\n",
    "    df_mode_pivot_table = df_mode_pivot_table.sort_values(by=['user_id', 'instance_id'], ascending=False)\n",
    "    df_mode_pivot_table = df_mode_pivot_table.drop_duplicates(['user_id'])\n",
    "    df_mode_pivot_table.rename(columns={colName:colName + '_mode'}, inplace=True)\n",
    "\n",
    "    train_df_1 = pd.merge(train_df_1, df_user_item_pivot_table, on=['user_id'], how='left')\n",
    "    train_df_1 = pd.merge(train_df_1, df_mode_pivot_table[['user_id', colName + '_mode']], on=['user_id'], how='left')\n",
    "    train_df_2 = pd.merge(train_df_2, df_user_item_pivot_table, on=['user_id'], how='left')\n",
    "    train_df_2 = pd.merge(train_df_2, df_mode_pivot_table[['user_id', colName + '_mode']], on=['user_id'], how='left')\n",
    "    train_df_3 = pd.merge(train_df_3, df_user_item_pivot_table, on=['user_id'], how='left')\n",
    "    train_df_3 = pd.merge(train_df_3, df_mode_pivot_table[['user_id', colName + '_mode']], on=['user_id'], how='left')\n",
    "    test_df = pd.merge(test_df, df_user_item_pivot_table, on=['user_id'], how='left')\n",
    "    test_df = pd.merge(test_df, df_mode_pivot_table[['user_id', colName + '_mode']], on=['user_id'], how='left')\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserItemStatFuture(train_df_1, train_df_2, train_df_3, test_df, 'item_price_level')\n",
    "train_df_1, train_df_2, train_df_3, test_df = getUserItemStatFuture(train_df_1, train_df_2, train_df_3, test_df, 'item_sales_level')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          last_user_item_id                date  is_later_clickSameItem\n",
      "10951913              False 2018-09-04 20:51:45                       0\n",
      "10951912               True 2018-09-04 20:45:17                       1\n",
      "10951911              False 2018-09-04 22:20:24                       0\n",
      "10951910              False 2018-09-04 22:16:15                       0\n",
      "10951909              False 2018-09-06 09:10:18                       0\n",
      "10951908              False 2018-09-05 08:07:44                       0\n",
      "10951907              False 2018-09-05 08:04:53                       0\n",
      "10951906               True 2018-09-05 07:17:07                       1\n",
      "10951905               True 2018-09-04 08:34:08                       1\n",
      "10951904               True 2018-09-04 08:28:27                       1\n",
      "10951903              False 2018-09-04 08:32:28                       0\n",
      "10951902              False 2018-09-05 19:02:34                       0\n",
      "10951901              False 2018-09-06 08:58:40                       0\n",
      "10951900              False 2018-09-05 12:15:06                       0\n",
      "10951899              False 2018-09-06 08:14:59                       0\n",
      "10951898              False 2018-09-05 08:07:44                       0\n",
      "10951897              False 2018-09-06 09:00:26                       0\n",
      "10951896              False 2018-09-05 18:51:44                       0\n",
      "10951895              False 2018-09-05 19:02:34                       0\n",
      "10951894              False 2018-09-05 19:03:23                       0\n",
      "6220199\n"
     ]
    }
   ],
   "source": [
    "# 统计历史记录中某用户后面是否有点击相同商品\n",
    "def getIsClickSameItemLater(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_id_str'] = train_df_1_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_id_str'] = train_df_2_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_id_str'] = train_df_3_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_id_str'] = test_df_copy['item_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_id'] = test_df_copy['user_id_str'] + test_df_copy['item_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_id', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf = tempDf.sort_values(by=['user_item_id', 'date'], ascending=False)\n",
    "    tempDf['last_user_item_id'] = tempDf['user_item_id'].shift(1)\n",
    "    tempDf['last_user_item_id'] = tempDf['last_user_item_id']==tempDf['user_item_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_id, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    for same, dt in tempDf[['last_user_item_id','date']].values:\n",
    "        if same:\n",
    "            historyShowList.append(1)\n",
    "        else:\n",
    "            historyShowList.append(0)\n",
    "    tempDf['is_later_clickSameItem'] = historyShowList\n",
    "    print(tempDf[['last_user_item_id', 'date', 'is_later_clickSameItem']].head(20))\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_id', 'date', 'is_later_clickSameItem']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_1['is_later_clickSameItem'] = train_df_1_copy['is_later_clickSameItem']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_id', 'date', 'is_later_clickSameItem']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_2['is_later_clickSameItem'] = train_df_2_copy['is_later_clickSameItem']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_id', 'date', 'is_later_clickSameItem']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_3['is_later_clickSameItem'] = train_df_3_copy['is_later_clickSameItem']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_id', 'date', 'is_later_clickSameItem']], on = ['user_item_id', 'date'], how='left')\n",
    "    test_df['is_later_clickSameItem'] = test_df_copy['is_later_clickSameItem']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getIsClickSameItemLater(train_df_1, train_df_2, train_df_3, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计历史记录中某用户后面是否有点击相同叶子类目商品\n",
    "def getIsClickSameLastCategoryLater(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_last_category_str'] = train_df_1_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_last_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_last_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_last_category_str'] = train_df_2_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_last_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_last_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_last_category_str'] = train_df_3_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_last_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_last_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_last_category_str'] = test_df_copy['real_last_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_last_category'] = test_df_copy['user_id_str'] + test_df_copy['real_last_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    \n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_last_category', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf = tempDf.sort_values(by=['user_real_last_category', 'date'], ascending=False)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['user_real_last_category'].shift(1)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['last_user_real_last_category']==tempDf['user_real_last_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_last_category, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    for same, dt in tempDf[['last_user_real_last_category','date']].values:\n",
    "        if same:\n",
    "            historyShowList.append(1)\n",
    "        else:\n",
    "            historyShowList.append(0)\n",
    "    tempDf['is_later_clickSameLastCategory'] = historyShowList\n",
    "    \n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_last_category', 'date', 'is_later_clickSameLastCategory']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_1['is_later_clickSameLastCategory'] = train_df_1_copy['is_later_clickSameLastCategory']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_last_category', 'date', 'is_later_clickSameLastCategory']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_2['is_later_clickSameLastCategory'] = train_df_2_copy['is_later_clickSameLastCategory']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_last_category', 'date', 'is_later_clickSameLastCategory']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_3['is_later_clickSameLastCategory'] = train_df_3_copy['is_later_clickSameLastCategory']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_last_category', 'date', 'is_later_clickSameLastCategory']], on = ['user_real_last_category', 'date'], how='left')\n",
    "    test_df['is_later_clickSameLastCategory'] = test_df_copy['is_later_clickSameLastCategory']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getIsClickSameLastCategoryLater(train_df_1, train_df_2, train_df_3, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    }
   ],
   "source": [
    "# 统计历史记录中某用户后面点击相同商品的个数\n",
    "def getClickSameItemLaterNumber(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_id_str'] = train_df_1_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_id_str'] = train_df_2_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_id_str'] = train_df_3_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_id_str'] = test_df_copy['item_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_id'] = test_df_copy['user_id_str'] + test_df_copy['item_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_id', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf = tempDf.sort_values(by=['user_item_id', 'date'], ascending=False)\n",
    "    tempDf['last_user_item_id'] = tempDf['user_item_id'].shift(1)\n",
    "    tempDf['last_user_item_id'] = tempDf['last_user_item_id']==tempDf['user_item_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_id, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    historyShowTemp = {}\n",
    "    for same, dt in tempDf[['last_user_item_id','date']].values:\n",
    "        if same:\n",
    "            if len(historyShowTemp) > 0:\n",
    "                historyShowList.append(len(historyShowTemp))\n",
    "                historyShowTemp[dt] = same\n",
    "            else:\n",
    "                historyShowList.append(0)\n",
    "                historyShowTemp[dt] = same\n",
    "        else:\n",
    "            historyShowList.append(0)\n",
    "            historyShowTemp = {dt:same}\n",
    "    tempDf['later_clickSameItem_count'] = historyShowList\n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_id', 'date', 'later_clickSameItem_count']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_1['later_clickSameItem_count'] = train_df_1_copy['later_clickSameItem_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_id', 'date', 'later_clickSameItem_count']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_2['later_clickSameItem_count'] = train_df_2_copy['later_clickSameItem_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_id', 'date', 'later_clickSameItem_count']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_3['later_clickSameItem_count'] = train_df_3_copy['later_clickSameItem_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_id', 'date', 'later_clickSameItem_count']], on = ['user_item_id', 'date'], how='left')\n",
    "    test_df['later_clickSameItem_count'] = test_df_copy['later_clickSameItem_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getClickSameItemLaterNumber(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计历史记录中某用户后面点击相同叶子类目商品的个数\n",
    "def getClickSameLastCategoryLaterNumber(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_last_category_str'] = train_df_1_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_last_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_last_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_last_category_str'] = train_df_2_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_last_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_last_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_last_category_str'] = train_df_3_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_last_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_last_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_last_category_str'] = test_df_copy['real_last_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_last_category'] = test_df_copy['user_id_str'] + test_df_copy['real_last_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    \n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_last_category', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf = tempDf.sort_values(by=['user_real_last_category', 'date'], ascending=False)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['user_real_last_category'].shift(1)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['last_user_real_last_category']==tempDf['user_real_last_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_last_category, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    historyShowTemp = {}\n",
    "    for same, dt in tempDf[['last_user_real_last_category','date']].values:\n",
    "        if same:\n",
    "            if len(historyShowTemp) > 0:\n",
    "                historyShowList.append(len(historyShowTemp))\n",
    "                historyShowTemp[dt] = same\n",
    "            else:\n",
    "                historyShowList.append(0)\n",
    "                historyShowTemp[dt] = same\n",
    "        else:\n",
    "            historyShowList.append(0)\n",
    "            historyShowTemp = {dt:same}\n",
    "    tempDf['later_clickSameLastCategory_count'] = historyShowList\n",
    "    \n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_count']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_1['later_clickSameLastCategory_count'] = train_df_1_copy['later_clickSameLastCategory_count']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_count']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_2['later_clickSameLastCategory_count'] = train_df_2_copy['later_clickSameLastCategory_count']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_count']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_3['later_clickSameLastCategory_count'] = train_df_3_copy['later_clickSameLastCategory_count']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_count']], on = ['user_real_last_category', 'date'], how='left')\n",
    "    test_df['later_clickSameLastCategory_count'] = test_df_copy['later_clickSameLastCategory_count']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getClickSameLastCategoryLaterNumber(train_df_1, train_df_2, train_df_3, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/lab-zhao.yinhu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220199\n"
     ]
    }
   ],
   "source": [
    "# 统计历史记录中某用户后面点击相同商品的时间间隔\n",
    "def getClickSameItemLaterDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'item_id', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['item_id_str'] = train_df_1_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_item_id'] = train_df_1_copy['user_id_str'] + train_df_1_copy['item_id_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['item_id_str'] = train_df_2_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_item_id'] = train_df_2_copy['user_id_str'] + train_df_2_copy['item_id_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['item_id_str'] = train_df_3_copy['item_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_item_id'] = train_df_3_copy['user_id_str'] + train_df_3_copy['item_id_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['item_id_str'] = test_df_copy['item_id'].map(lambda x: str(x))\n",
    "    test_df_copy['user_item_id'] = test_df_copy['user_id_str'] + test_df_copy['item_id_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_item_id', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf = tempDf.sort_values(by=['user_item_id', 'date'], ascending=False)\n",
    "    tempDf['last_user_item_id'] = tempDf['user_item_id'].shift(1)\n",
    "    tempDf['last_user_item_id'] = tempDf['last_user_item_id']==tempDf['user_item_id']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_item_id, 'last_show_time'] = np.nan\n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_item_id', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['later_clickSameItem_deltaTime'] = historyShowList\n",
    "    \n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_item_id', 'date', 'later_clickSameItem_deltaTime']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_1['later_clickSameItem_deltaTime'] = train_df_1_copy['later_clickSameItem_deltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_item_id', 'date', 'later_clickSameItem_deltaTime']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_2['later_clickSameItem_deltaTime'] = train_df_2_copy['later_clickSameItem_deltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_item_id', 'date', 'later_clickSameItem_deltaTime']], how='left', on=['user_item_id', 'date'])\n",
    "    train_df_3['later_clickSameItem_deltaTime'] = train_df_3_copy['later_clickSameItem_deltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_item_id', 'date', 'later_clickSameItem_deltaTime']], on = ['user_item_id', 'date'], how='left')\n",
    "    test_df['later_clickSameItem_deltaTime'] = test_df_copy['later_clickSameItem_deltaTime']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getClickSameItemLaterDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计历史记录中某用户后面点击相同叶子类目商品的时间间隔\n",
    "def getClickSameLastCategoryLaterDeltaTime(train_df_1, train_df_2, train_df_3, test_df):\n",
    "    print(len(train_df_1))\n",
    "    train_df_1_copy = train_df_1[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_2_copy = train_df_2[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_3_copy = train_df_3[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    test_df_copy = test_df[['user_id', 'real_last_category', 'date', 'instance_id']]\n",
    "    train_df_1_copy['user_id_str'] = train_df_1_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_1_copy['real_last_category_str'] = train_df_1_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_1_copy['user_real_last_category'] = train_df_1_copy['user_id_str'] + train_df_1_copy['real_last_category_str']\n",
    "    train_df_2_copy['user_id_str'] = train_df_2_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_2_copy['real_last_category_str'] = train_df_2_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_2_copy['user_real_last_category'] = train_df_2_copy['user_id_str'] + train_df_2_copy['real_last_category_str']\n",
    "    train_df_3_copy['user_id_str'] = train_df_3_copy['user_id'].map(lambda x: str(x))\n",
    "    train_df_3_copy['real_last_category_str'] = train_df_3_copy['real_last_category'].map(lambda x: str(x))\n",
    "    train_df_3_copy['user_real_last_category'] = train_df_3_copy['user_id_str'] + train_df_3_copy['real_last_category_str']\n",
    "    test_df_copy['user_id_str'] = test_df_copy['user_id'].map(lambda x: str(x))\n",
    "    test_df_copy['real_last_category_str'] = test_df_copy['real_last_category'].map(lambda x: str(x))\n",
    "    test_df_copy['user_real_last_category'] = test_df_copy['user_id_str'] + test_df_copy['real_last_category_str']\n",
    "    temp_df = pd.concat([train_df_1_copy, train_df_2_copy, train_df_3_copy, test_df_copy])\n",
    "    \n",
    "    tempDf = pd.pivot_table(temp_df, index=['user_real_last_category', 'date'], values=['instance_id'], aggfunc=len)\n",
    "    tempDf.columns = ['show']\n",
    "    tempDf.reset_index(inplace=True)\n",
    "    tempDf = tempDf.sort_values(by=['user_real_last_category', 'date'], ascending=False)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['user_real_last_category'].shift(1)\n",
    "    tempDf['last_user_real_last_category'] = tempDf['last_user_real_last_category']==tempDf['user_real_last_category']\n",
    "    tempDf['last_show_time'] = tempDf['date'].shift(1)\n",
    "    tempDf.loc[~tempDf.last_user_real_last_category, 'last_show_time'] = np.nan\n",
    "    \n",
    "    historyShowList = []\n",
    "    deltaTime = 99999999\n",
    "    for same, dt, lastShowTime in tempDf[['last_user_real_last_category', 'date', 'last_show_time']].values:\n",
    "        if same:\n",
    "            deltaTime = (dt - lastShowTime) / np.timedelta64(1, 's')\n",
    "            historyShowList.append(deltaTime)\n",
    "            deltaTime = 99999999\n",
    "        else:\n",
    "            historyShowList.append(deltaTime)\n",
    "    tempDf['later_clickSameLastCategory_deltaTime'] = historyShowList\n",
    "    \n",
    "    train_df_1_copy = train_df_1_copy.merge(tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_deltaTime']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_1['later_clickSameLastCategory_deltaTime'] = train_df_1_copy['later_clickSameLastCategory_deltaTime']\n",
    "    train_df_2_copy = train_df_2_copy.merge(tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_deltaTime']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_2['later_clickSameLastCategory_deltaTime'] = train_df_2_copy['later_clickSameLastCategory_deltaTime']\n",
    "    train_df_3_copy = train_df_3_copy.merge(tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_deltaTime']], how='left', on=['user_real_last_category', 'date'])\n",
    "    train_df_3['later_clickSameLastCategory_deltaTime'] = train_df_3_copy['later_clickSameLastCategory_deltaTime']\n",
    "    test_df_copy = pd.merge(test_df_copy, tempDf[['user_real_last_category', 'date', 'later_clickSameLastCategory_deltaTime']], on = ['user_real_last_category', 'date'], how='left')\n",
    "    test_df['later_clickSameLastCategory_deltaTime'] = test_df_copy['later_clickSameLastCategory_deltaTime']\n",
    "    print(len(train_df_1))\n",
    "    return train_df_1, train_df_2, train_df_3, test_df\n",
    "\n",
    "\n",
    "train_df_1, train_df_2, train_df_3, test_df = getClickSameLastCategoryLaterDeltaTime(train_df_1, train_df_2, train_df_3, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导出还没做好滑窗处理的数据，包括训练集和测试集\n",
    "# 导出训练集划分结果\n",
    "def exportResult(df, fileName):\n",
    "    df.to_csv('~/kengkeng/alimama/data/%s.csv' % fileName, header=True, index=False)\n",
    "    \n",
    "train_df_all = pd.concat([train_df_1, train_df_2, train_df_3])\n",
    "\n",
    "exportResult(train_df, 'fusai_a_train_df_all')\n",
    "exportResult(test_df, 'fusai_a_test_df_all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#尝试统计两天前滑窗结果，包括商品，店铺，品牌，叶子类目的转化率，点击次数和购买次数\n",
    "#首先划分数据集\n",
    "future_dataset1 = train_df_1[(train_df_1.day == 31) | (train_df_1.day == 1)]\n",
    "train_df_huachuang_1 = train_df_1[train_df_1.day == 2]\n",
    "future_dataset2 = train_df_1[(train_df_1.day == 1) | (train_df_1.day == 2)]\n",
    "train_df_huachuang_2 = train_df_1[train_df_1.day == 3]\n",
    "future_dataset3 = train_df_1[(train_df_1.day == 2) | (train_df_1.day == 3)]\n",
    "train_df_huachuang_3 = train_df_1[train_df_1.day == 4]\n",
    "future_dataset4 = train_df_2\n",
    "train_df_huachuang_4 = train_df_3\n",
    "\n",
    "def getBaseConversionRate(future_df, test_df, colName):\n",
    "    t = future_df[[colName]]\n",
    "    t[colName + '_total_number'] = 1\n",
    "    t = t.groupby(colName).agg('sum').reset_index()\n",
    "\n",
    "    t_buy = future_df[[colName]][future_df.is_trade == 1]\n",
    "    t_buy[colName + '_buy_number'] = 1\n",
    "    t_buy = t_buy.groupby(colName).agg('sum').reset_index()\n",
    "\n",
    "    t = pd.merge(t, t_buy, on=colName, how='left')\n",
    "    t[colName + '_buy_number'] = t[colName + '_buy_number'].map(lambda x: 0 if math.isnan(x) else x)\n",
    "    t['buy_origion_rate'] = t[colName + '_buy_number'] / t[colName + '_total_number']\n",
    "    alpha, beta = getBayesSmoothParam(t['buy_origion_rate'])\n",
    "    t[colName + '_converse_smooth_rate'] = (t[colName + '_buy_number'] + alpha) / (t[colName + '_total_number'] + alpha + beta)\n",
    "#     train_df = pd.merge(train_df, t[[colName, colName + '_converse_smooth_rate']], on=colName, how='left')\n",
    "#     train_df[colName + '_converse_smooth_rate'] = train_df[colName + '_converse_smooth_rate'].map(lambda x: (alpha / (alpha + beta)) if math.isnan(x) else x)\n",
    "    \n",
    "    test_df = pd.merge(test_df, t[[colName, colName + '_converse_smooth_rate', colName + '_total_number', colName + '_buy_number']], on=colName, how='left')\n",
    "    test_df[colName + '_converse_smooth_rate'] = test_df[colName + '_converse_smooth_rate'].map(lambda x: (alpha / (alpha + beta)) if math.isnan(x) else x)\n",
    "    \n",
    "    test_df[colName + '_total_number'] = test_df[colName + '_total_number'].fillna(0)\n",
    "    test_df, total_number_scaler = scalerFea(test_df, colName + '_total_number')\n",
    "    \n",
    "    test_df[colName + '_buy_number'] = test_df[colName + '_buy_number'].fillna(0)\n",
    "    test_df, buy_number_scaler = scalerFea(test_df, colName + '_buy_number')\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "#定义对每个窗口进行操作的函数\n",
    "def dealHuaChuangDataset(future_df, dataset):\n",
    "    dataset = getBaseConversionRate(future_df, dataset, 'user_id')\n",
    "    dataset = getBaseConversionRate(future_df, dataset, 'item_brand_id')\n",
    "    dataset = getBaseConversionRate(future_df, dataset, 'item_id')\n",
    "    dataset = getBaseConversionRate(future_df, dataset, 'shop_id')\n",
    "    dataset = getBaseConversionRate(future_df, dataset, 'real_last_category')\n",
    "    return dataset\n",
    "\n",
    "print(len(test_df.columns))\n",
    "train_df_huachuang_1 = dealHuaChuangDataset(future_dataset1, train_df_huachuang_1)\n",
    "train_df_huachuang_2 = dealHuaChuangDataset(future_dataset2, train_df_huachuang_2)\n",
    "train_df_huachuang_3 = dealHuaChuangDataset(future_dataset3, train_df_huachuang_3)\n",
    "train_df_huachuang_4 = dealHuaChuangDataset(future_dataset4, train_df_huachuang_4)\n",
    "test_df = dealHuaChuangDataset(future_dataset4, test_df)\n",
    "print(len(test_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将2,3,4号数据抽样15%与7号当天数据结合作为训练接，拟合线上分布\n",
    "train_df_234 = pd.concat([train_df_huachuang_1, train_df_huachuang_2, train_df_huachuang_3])\n",
    "train_df_234 = train_df_234.sample(frac = 0.15, replace = True)\n",
    "\n",
    "print(len(train_df_234))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df_huachuang_4, train_df_234])\n",
    "print(len(train_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exportResult(train_df, 'fusai_a_train_df_weilai')\n",
    "exportResult(test_df, 'fusai_a_test_df_weilai')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(test_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
